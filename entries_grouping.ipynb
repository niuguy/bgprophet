{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing -1\n",
    "Aggregation to each person "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_dataframe(patient_id, file_name):\n",
    "    with open(file_name) as file:\n",
    "        j_object = json.load(file)\n",
    "        df = pd.DataFrame(j_object)\n",
    "#         try:\n",
    "#             df['datetime'] = pd.Timestamp(df['dateString']).to_datetime()\n",
    "        df['datetime'] = pd.to_datetime(df['dateString'], utc=True)\n",
    "#         except ValueError:\n",
    "# #             df['datetime'] = pd.to_datetime(df['dateString'],format=\"%d/%m/%Y %H:%M:%S %p\")\n",
    "#         else:\n",
    "#             raise\n",
    "        \n",
    "        df['pid'] = patient_id\n",
    "        df_result = df[['pid', 'datetime', 'sgv']]\n",
    "        df_result = df_result.sort_values(by=['datetime'], ascending = True)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for folder in os.listdir(os.getcwd()+'/OpenAPS'):\n",
    "    print('PID:', folder)\n",
    "    record_path = 'OpenAPS/'+folder+'/direct-sharing-31'\n",
    "    if os.path.exists(record_path):\n",
    "        files = os.listdir(record_path)\n",
    "        df_folder = pd.DataFrame([], columns=['pid', 'datetime', 'sgv'])\n",
    "        for file in files:\n",
    "            if file.find('entries') > -1 and file.endswith('.json'):\n",
    "                try:\n",
    "                    df = transfer_dataframe(folder, record_path+'/'+file)\n",
    "                except :\n",
    "                    print(folder, file, 'cannot be transfered to datetime, pass' )\n",
    "                    continue\n",
    "                  \n",
    "                df_folder = df_folder.append(df)\n",
    "        print('before drop', len(df_folder))\n",
    "        df_folder.drop_duplicates('datetime',inplace = True)\n",
    "        print('after drop', len(df_folder))\n",
    "        print('saving result...')\n",
    "        pickle.dump(df_folder, open(os.getcwd()+'/OpenAPS/entry_dfs/'+folder+'_entries.pkl', 'wb'))\n",
    "        print('done for', folder)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-09-10 19:39:21')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    pd.to\n",
    "pd.to_datetime('10/09/2016 19:39:21 PM',format=\"%d/%m/%Y %H:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-10-08 23:58:22.681000+0000', tz='UTC')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('2017-10-08T19:58:22.681-0400', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.getcwd()+'/OpenAPS/entry_dfs'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/OpenAPS/entry_dfs'\n",
    "for file in os.listdir(path):\n",
    "    df = pickle.load(open(path+'/'+file, 'rb'))\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing -2\n",
    "Find same time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for file in os.listdir(path):\n",
    "    df = pickle.load(open(path+'/'+file, 'rb'))\n",
    "#     mask = (df['datetime'] > '2017-03-01') & (df['datetime'] < '2017-03-11')\n",
    "    df.set_index('datetime',inplace = True)\n",
    "#     df.index = df.tz_convert('America/Los_Angeles')\n",
    "    df = df.tz_convert(None)\n",
    "\n",
    "    df_selected = df['2017-03-01':'2017-03-11']\n",
    "    if len(df_selected)>0:\n",
    "        print(df_selected.index[:2])\n",
    "        print(df_selected['pid'].values[0])\n",
    "        pickle.dump(df_selected, open(os.getcwd()+'/OpenAPS/entry_dfs_sliced/'+df_selected['pid'].values[0]+'.pkl', 'wb'))\n",
    "        count += 1\n",
    "print('Instances found:', count)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = os.getcwd()+'/OpenAPS/entry_dfs_sliced/'\n",
    "\n",
    "files = os.listdir(path2)\n",
    "\n",
    "df1 = pickle.load(open(path2+'/'+files[0], 'rb'))\n",
    "df2 = pickle.load(open(path2+'/'+files[1], 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distance with Dynamic Time Wrappinng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def DTWDistance(s1, s2,w):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            \n",
    "    return math.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LB_Keogh(s1,s2,r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "#         print('i', i)\n",
    "#         print('upper_bound', upper_bound)\n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return math.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365.276607518193"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTWDistance(df1['sgv'].values[:288], df2['sgv'].values[:288], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.03296106356203"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LB_Keogh(df1['sgv'].values[:288], df2['sgv'].values[:288], 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe into dicts\n",
    "def transfer_dicts(path, df_files):\n",
    "    rst = {}\n",
    "    for file in df_files:\n",
    "        df = pickle.load(open(path + file, 'rb'))        \n",
    "        \n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "rst = {}\n",
    "for file in os.listdir(path):\n",
    "    df = pickle.load(open(path+'/'+file, 'rb'))\n",
    "    df.set_index('datetime',inplace = True)\n",
    "    df = df.tz_convert(None)\n",
    "    df.dropna(inplace=True)\n",
    "    if len(df)==0:\n",
    "        continue\n",
    "    pid =np.asscalar(df['pid'].values[:1])\n",
    "#     print('pid', pid)\n",
    "    bgs = df.sgv.values[:288]\n",
    "#     print('len of bgs', len(bgs))\n",
    "    if len(bgs)>0:\n",
    "        rst[int(pid)] = bgs\n",
    "\n",
    "print(len(rst))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means_clust(bg_dicts,num_clust,num_iter,w=5):\n",
    "    keys = list(bg_dicts.keys())\n",
    "    # for the final result backtracking\n",
    "    search_dict = {}\n",
    "    for i in range(len(keys)):\n",
    "        search_dict[i] = keys[i]\n",
    "    data = list(bg_dicts.values())\n",
    "    data = [[int(l) for l in r] for r in data]\n",
    "    centroids = random.sample(data, num_clust)\n",
    "       \n",
    "    counter=0\n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                lb = LB_Keogh(i,j,5)\n",
    "#                 print('lb', lb)\n",
    "#                 print('min_dist', min_dist)\n",
    "                if lb<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "#             if closest_clust in assignments:\n",
    "#                 assignments[closest_clust].append(ind)\n",
    "#             else:\n",
    "#                 assignments[closest_clust]=[]                \n",
    "            assignments.setdefault(closest_clust,[])\n",
    "            assignments[closest_clust].append(ind)\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            clust_sum=np.zeros(len(data[0]))\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+data[k]\n",
    "                \n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "    \n",
    "    return centroids,assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "centroids, assignnments = k_means_clust(rst, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: [0, 5, 10, 14, 15, 16, 17, 19, 21, 25, 26, 29, 33, 36, 45, 48, 49, 53, 56, 57, 60, 62, 67, 68, 75, 76, 77], 0: [1, 18, 20, 30, 32, 35, 37, 39, 47, 50, 52, 58, 64, 65], 1: [2, 3, 6, 8, 11, 12, 23, 24, 27, 28, 31, 34, 38, 40, 41, 42, 43, 61, 66, 70, 71, 72, 78], 2: [4, 7, 9, 13, 22, 44, 46, 51, 54, 55, 59, 63, 69, 73, 74]}\n"
     ]
    }
   ],
   "source": [
    "print(assignnments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57176789 [27700103, 84984656, 89710417, 897741, 28768536, 1884126, 1352464, 80373992, 89727223, 13029224, 66836068, 13484299, 32407882, 17161370, 7886752, 46966807, 98974339, 74077367, 14092221, 99296581, 62401782, 24587372, 45025419, 37948668, 39986716, 87770486, 88004055]\n",
      "27700103 [96254963, 3572116, 67208817, 97417885, 67359234, 5274556, 47750728, 96484928, 60844515, 37764532, 85199788, 96805916, 21946407, 71397255]\n",
      "96254963 [73521474, 57176789, 71236754, 49796612, 28176124, 40237051, 80501215, 24448124, 66019205, 95851255, 20216809, 84081904, 78420229, 77104076, 94875538, 41663654, 13783771, 95614431, 72492570, 99848889, 91161972, 38110191, 64024750]\n",
      "73521474 [51336122, 43104037, 20396154, 79526193, 99908129, 37875431, 84589080, 43589707, 4762925, 34148224, 40997757, 84109428, 25692073, 15634563, 80796147]\n"
     ]
    }
   ],
   "source": [
    "keys = list(rst.keys())\n",
    "clusters = {}\n",
    "for k,v in assignnments.items():\n",
    "    clusters[keys[k]] = [keys[i] for i in v]\n",
    "\n",
    "for k,v in clusters.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
